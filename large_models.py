import base64
import os
import asyncio
import json
from datetime import datetime

from openai import AsyncOpenAI

from models import FileModel, ContentModel
from decoder import PDFDecoder
from config import config
from prompts import Prompts
from logger import setup_logger

logger = setup_logger(__name__, use_emoji=True)

class IMGRecognizer:
    def __init__(self):
        self.file_db = FileModel()
        self.content_db = ContentModel()
        self.decoder = PDFDecoder()
        self.vlm_client = AsyncOpenAI(
            api_key=config.vlm_api_key,
            base_url=config.vlm_base_url,
        )
        self.model = config.vlm_model_name

    async def image_understanding(self):
        """è°ƒç”¨qwen2.5-vl-72b-instructæ¨¡å‹ï¼Œç”Ÿæˆæ»¡è¶³pagesä¸­æ‰€æœ‰ä¿¡æ¯"""
        # è·å–éœ€å¤„ç†çš„é¡µé¢
        file_pages = await self._get_unidentified()
        if not file_pages:
            logger.info("æ²¡æœ‰éœ€è¦å¤„ç†çš„é¡µé¢")
            return

        # å±•å¼€æ‰€æœ‰å¾…å¤„ç†é¡µé¢
        all_pages = []
        for file_group in file_pages:
            for page_info in file_group["info"]:
                all_pages.append({
                    "file_id": file_group["file_id"],
                    "page_number": page_info["page_number"],
                    "page_path": page_info["page_path"]
                })

        logger.info(f"ğŸ“– å¼€å§‹å¤„ç†æ–‡ä»¶ä»½æ•°ï¼š[{len(file_pages)}]ä»½ï¼Œå…± {len(all_pages)} é¡µ")

        # åˆ›å»ºä¿¡å·é‡æ§åˆ¶å¹¶å‘æ•°
        semaphore = asyncio.Semaphore(5)

        # åˆ›å»ºä»»åŠ¡åˆ—è¡¨
        tasks = []
        for page in all_pages:
            task = asyncio.create_task(
                self._process_page_with_retry(page, semaphore)
            )
            tasks.append(task)

        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # å¤„ç†ç»“æœ
        success_count = 0
        for result in results:
            if not isinstance(result, Exception) and result:
                success_count += 1

        logger.info(f"âœ… å¤„ç†å®Œæˆï¼æˆåŠŸå¤„ç†é¡µæ•°: {success_count}é¡µï¼Œå¤±è´¥é¡µæ•°: {len(results) - success_count}é¡µ")

    async def _process_page_with_retry(self, page: dict, semaphore: asyncio.Semaphore):
        """å¸¦é‡è¯•æœºåˆ¶çš„é¡µé¢å¤„ç†"""
        max_retries = 3
        retry_delay = 2.0  # åˆå§‹å»¶è¿Ÿ2ç§’

        async with semaphore:
            for attempt in range(max_retries):
                try:
                    # è°ƒç”¨å¤§æ¨¡å‹å¤„ç†é¡µé¢
                    ai_response = await self.process_page(page)
                    if not ai_response:
                        raise ValueError("AIè¿”å›ç©ºå“åº”")

                    # è§£æAIè¿”å›çš„JSON
                    ai_data = IMGRecognizer._parse_ai_response(ai_response)
                    if not ai_data:
                        raise ValueError("AIå“åº”è§£æå¤±è´¥")

                    # æ›´æ–°æ•°æ®åº“
                    await self._update_models(page, ai_data)
                    return True

                except Exception as e:
                    logger.warning(f"é¡µé¢å¤„ç†å°è¯• {attempt + 1}/{max_retries} å¤±è´¥ | "
                                   f"æ–‡ä»¶: {page['file_id']} é¡µ: {page['page_number']} | "
                                   f"é”™è¯¯: {str(e)}")

                    # æœ€åä¸€æ¬¡å°è¯•ä»ç„¶å¤±è´¥
                    if attempt == max_retries - 1:
                        logger.error(f"âŒ é¡µé¢å¤„ç†å¤±è´¥ | "
                                     f"æ–‡ä»¶: {page['file_id']} é¡µ: {page['page_number']} | "
                                     f"æœ€ç»ˆé”™è¯¯: {str(e)}")
                        return False

                    # æŒ‡æ•°é€€é¿é‡è¯•
                    await asyncio.sleep(retry_delay * (2 ** attempt))

        return False
    @staticmethod
    def _parse_ai_response(ai_response: str) -> dict:
        """è§£æAIè¿”å›çš„JSONå“åº”"""
        try:
            # æå–JSONéƒ¨åˆ†ï¼ˆå¯èƒ½åŒ…å«éJSONå‰ç¼€ï¼‰
            json_start = ai_response.find('{')
            json_end = ai_response.rfind('}') + 1
            json_str = ai_response[json_start:json_end]

            return json.loads(json_str)
        except Exception as e:
            logger.error(f"JSONè§£æå¤±è´¥: {e}\nåŸå§‹å“åº”: {ai_response}\n")
            return {}

    async def _update_models(self, page: dict, ai_data: dict):
        """æ›´æ–°FileModelå’ŒContentModel"""
        file_id = page["file_id"]
        page_number = page["page_number"]
        now = datetime.now().isoformat()

        # 1. æ›´æ–°FileModelä¸­çš„é¡µé¢çŠ¶æ€
        file_record = self.file_db.get_file_by_id(file_id)
        if not file_record:
            raise ValueError(f"æ–‡ä»¶IDä¸å­˜åœ¨: {file_id}")

        # åœ¨æ–‡ä»¶è®°å½•çš„pagesæ•°ç»„ä¸­æŸ¥æ‰¾å¹¶æ›´æ–°å¯¹åº”é¡µé¢
        updated_pages = []
        page_found = False

        for p in file_record["pages"]:
            if p["page_number"] == page_number:
                # æ›´æ–°é¡µé¢çŠ¶æ€
                p.update({
                    "is_aigc": True,
                    "processed_at": now,
                    "property": ai_data.get("property", ""),
                    "title": ai_data.get("title", ""),
                    "abstract": ai_data.get("abstract", ""),
                    "keywords": ai_data.get("keywords", [])
                })
                page_found = True
            updated_pages.append(p)

        if not page_found:
            raise ValueError(f"æ–‡ä»¶ä¸­æœªæ‰¾åˆ°é¡µç : {page_number}")

        # æ›´æ–°æ•´ä¸ªpagesæ•°ç»„
        self.file_db.files.update(
            {"pages": updated_pages},
            self.file_db.query.file_id == file_id
        )

        # 2. æ›´æ–°æ–‡ä»¶çº§åˆ«çš„tagså’Œfile_descå­—æ®µ
        # é‡æ–°è·å–æ›´æ–°åçš„æ–‡ä»¶è®°å½•ï¼ˆåŒ…å«æ‰€æœ‰é¡µé¢æœ€æ–°æ•°æ®ï¼‰
        updated_file_record = self.file_db.get_file_by_id(file_id)
        if not updated_file_record:
            raise ValueError(f"æ›´æ–°åæ–‡ä»¶IDä¸å­˜åœ¨: {file_id}")

        # æ”¶é›†æ‰€æœ‰é¡µé¢çš„keywords
        all_keywords = []
        for p in updated_file_record["pages"]:
            if p.get("keywords") and isinstance(p["keywords"], list):
                all_keywords.extend(p["keywords"])

        # å»é‡å¹¶è¿‡æ»¤ç©ºå€¼
        unique_keywords = list(set(filter(None, all_keywords)))

        # æ”¶é›†æ‰€æœ‰é¡µé¢çš„abstractï¼ˆæŒ‰é¡µç æ’åºï¼‰
        abstracts = []
        for p in sorted(updated_file_record["pages"], key=lambda x: x["page_number"]):
            if p.get("abstract"):
                abstracts.append(p["abstract"])

        # ç”¨æ¢è¡Œç¬¦è¿æ¥abstracts
        file_desc = "\n".join(abstracts)

        # æ›´æ–°æ–‡ä»¶è®°å½•çš„tagså’Œfile_descå­—æ®µ
        self.file_db.files.update(
            {
                "tags": unique_keywords,
                "file_desc": file_desc
            },
            self.file_db.query.file_id == file_id
        )

        # 3. åˆ›å»º/æ›´æ–°ContentModelè®°å½•
        # åˆ›å»ºæ–°è®°å½•æ—¶éœ€è¦çš„æ•°æ®
        content_data = {
            "file_id": file_id,
            "page_number": page_number,
            "content": ai_data.get("content", ""),
            "title": ai_data.get("title", ""),  # æ–°å¢å­—æ®µ
            "prop": ai_data.get("property", ""),  # æ–°å¢å­—æ®µ
            "abstract": ai_data.get("abstract", ""),  # æ–°å¢å­—æ®µ
            "keywords": ai_data.get("keywords", []),
            "created_at": now  # åªåœ¨åˆ›å»ºæ—¶ä½¿ç”¨
        }

        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨å†…å®¹è®°å½•
        existing_content = self.content_db.contents.get(
            (self.content_db.query.file_id == file_id) &
            (self.content_db.query.page_number == page_number)
        )

        if existing_content:
            # æ›´æ–°ç°æœ‰è®°å½• - ç§»é™¤åˆ›å»ºæ—¶é—´å­—æ®µ
            update_data = content_data.copy()
            update_data.pop("created_at", None)

            self.content_db.update_content(
                existing_content["page_id"],
                **update_data
            )
            logger.debug(f"å†…å®¹è®°å½•æ›´æ–° | æ–‡ä»¶: {file_id} é¡µ: {page_number}")
        else:
            # åˆ›å»ºæ–°è®°å½•ï¼ˆåŒ…å«æ‰€æœ‰å­—æ®µï¼‰
            self.content_db.create_content(**content_data)
            logger.debug(f"å†…å®¹è®°å½•åˆ›å»º | æ–‡ä»¶: {file_id} é¡µ: {page_number}")

        logger.info(f"âœ… æ›´æ–°å®Œæˆ | æ–‡ä»¶: {file_id} é¡µ: {page_number}")

    async def process_page(self, page: dict) -> str:
        """å¤„ç†å•ä¸ªé¡µé¢å¹¶è·å–AIæè¿°"""
        page_path = page.get("page_path")
        if not page_path or not os.path.exists(page_path):
            logger.warning(f"é¡µé¢å›¾ç‰‡ä¸å­˜åœ¨ï¼š{page_path}")
            return f"[ç¼ºå¤±é¡µé¢ {page.get('page_number')}]"
        # è·å–ä¼˜åŒ–åçš„å›¾åƒç¼–ç 
        base64_image = await self._image_to_base64(page_path)
        if not base64_image:
            return ""

        # è·å–æç¤ºæ¨¡æ¿
        try:
            prompt = Prompts.get_prompt(self.model)
            system_prompt = prompt["system"]
            user_message = prompt["user"]
        except ValueError as e:
            logger.error(f"æç¤ºè·å–å¤±è´¥: {str(e)}")
            return ""

        messages = [
            {
                "role": "system",
                "content": [{"type": "text", "text": system_prompt}]},
            {
                "role": "user",
                "content": [
                    {
                        "type": "image_url",
                        "image_url": {"url": f"data:image/jpeg;base64,{base64_image}"},
                    },
                    {
                        "type": "text",
                        "text": user_message
                    },
                ],
            }
        ]

        try:
            # è°ƒç”¨Qwen VLæ¨¡å‹
            response = await self.vlm_client.chat.completions.create(
                model=self.model,  # qwen-vl-72b-instruct
                messages=messages,  # type: ignore
                temperature=0.3,  # ä½æ¸©åº¦ç¡®ä¿è¾“å‡ºç¨³å®šæ€§
                max_tokens=2500  # åˆç†æ§åˆ¶tokené‡
            )

            content = response.choices[0].message.content
            return content

        except Exception as e:
            logger.error(f"é¡µé¢å¤„ç†å¤±è´¥ P{page['page_number']} | {str(e)}")
            return f"[å¤„ç†å¤±è´¥ P{page['page_number']}]"

    async def _get_unidentified(self):
        files = self.file_db.get_all_files()
        result = []
        for file in files:
            file_id = file.get("file_id", "æœªçŸ¥æ–‡ä»¶ID")
            pages = file.get("pages", [])
            item = [
                {
                    "page_number": page["page_number"],
                    "page_path": page["page_path"]
                }
                for page in pages if page.get("is_aigc") is False
            ]
            result.append({
                "file_id": file_id,
                "info": item
            })
        return result

    # å›¾åƒè½¬base64
    @staticmethod
    async def _image_to_base64(image_path: str) -> str:
        if not os.path.exists(image_path):
            logger.warning(f"å›¾ç‰‡ä¸å­˜åœ¨ï¼š{image_path}")
            return ""
        with open(image_path, "rb") as img_file:
            return base64.b64encode(img_file.read()).decode('utf-8')


if __name__ == "__main__":
    recognizer = IMGRecognizer()
    asyncio.run(recognizer.image_understanding())
